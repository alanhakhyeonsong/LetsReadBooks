# Chapter 1 - 대규모 웹 서비스 개발 오리엔테이션
## 대규모 서비스와 소규모 서비스
### 소규모 서비스와 대규모 서비스의 차이
서버 몇 대 정도의 소규모 서비스엔 없는, 대규모 서비스에만 있는 문제나 어려움엔 어떤 점들이 있을까?

- 확장성 확보, 부하분산 필요
  - 스케일 아웃이 이 문제에 대한 전략의 기초가 된다.
  - 스케일 업은 하드웨어의 성능을 높여 처리능력을 끌어올리는 방법이다.
  - 스케일 아웃 전략을 채용한 경우 비용이 절감되는 반면 다양한 문제가 발생한다.
    - 사용자로부터의 요청을 어떻게 분배할 것인가? → 로드밸런서 사용
    - DB 분산 시 데이터 동기화는 어떻게 할 것인가?
    - 네트워크 통신의 latency를 어떻게 생각해볼 수 있을까?
- 다중성 확보
  - 특정 서버가 고장나 성능이 저하되더라도 서비스를 계속 할 수 있는 구성으로 할 필요가 있다.
  - 서비스가 대규모화되면 될수록 시스템 정지의 사회적 충격도 늘어나무로 더욱 중요하다.
  - CDN 서비스로 컨텐츠를 캐싱해서 트래픽을 우회시킴으로 장애를 복구하는 사례가 많다.
  - 웹 서비스는 언제 어떠한 경우라도 고장에 대해 견고해야 하지만 상당히 어려운 태스크다.
- 효율적 운용 필요
  - 서버 대수가 100대를 넘어서면 어떤 서버가 무슨 역할을 하고 있는지 기억해두는 것조차 어렵다.
  - 각 서버가 어떤 상황에 있는지 파악하는 것도 상당한 고생거리다.
    - 부하는 괜찮은지, 고장 난 부분은 없는지, 디스크 용량은 충분한지, 보안설정에 미지한 점은 없는지 등
- 개발자 수, 개발방법의 변화
  - 늘어난 개발자 수에 따라 개발 표준화는 어떻게 할 것인가?
    - 프로그래밍 언어, 라이브러리/프레임워크 통일, 코딩 규약 표준화
    - 소스코드 버전 관리
  - 기술자 간 능력 차이에 따라 효율이 나쁜 부분은 생기지 않는지?
  - 교육 및 팀 매니지먼트는 어떻게 할 것인지?

### 대규모 데이터량에 대한 대처
컴퓨터는 디스크에서 데이터를 로드해 메모리에 저장, 메모리에 저장된 데이터를 CPU가 fetch해서 특정 처리를 수행한다. 또한 메모리에서 fetch된 명령은 보다 빠른 cache memory에 캐싱된다.

각 단계간 속도차가 매우 크게 나는 것이 현대 컴퓨터의 특징이다. 하드디스크에서 데이터를 읽어들이는 데에는 그 특성상 헤드 이동이나 디스크 원반의 회전이라는 물리적 동작이 수반된다. 따라서 전기적으로 읽어들이기만 하면 되는 메모리나 캐시 메모리와 비교하면 10^6 ~ 10^9배 정도 차이가 난다.
// 요즘은 HDD대신 SSD를 사용해 빨라졌지만 여전히 속도차가 나는 것은 맞다.

이 속도차를 흡수하기 위해 OS는 이런저런 방법을 사용하게 되는데, 디스크로부터 읽어들인 데이터를 메모리에 캐싱해둠으로써 전반적으로 디바이스간 속도차가 체감속도에 영향을 주지 않도록 하고 있다. DB를 비롯한 미들웨어도 기본적으로 이러한 속도차를 의식한 데이터 구조, 구현을 채용하고 있다.

하지만 OS나 미들웨어 등의 소프트웨어에서 이런 구조를 통해 분발한다곤해도 당연히 한계가 있다. 데이터량이 많아지면 캐시 미스가 많이 발생하게 되고, 그 결과로 저속의 디스크로의 I/O가 많이 발생하게 된다. 디스크 I/O 대기에 들어선 프로그램은 다른 리소스가 비어 있더라도 읽기가 완료되기까진 다음 처리를 수행할 수 없다. 이것이 시스템 전체의 속도저하를 초래한다.

데이터가 적을 때는 특별히 고민하지 않아도 모두 메모리에서 처리할 수 있으며, 복잡한 알고리즘을 사용하기보다 간단한 알고리즘을 사용하는 편이 오버헤드가 적기 때문에 더 빠른 경우도 종종 있으므로 I/O 부하는 일단 문제가 되지 않는다. 그러나 서비스가 어느 정도 이상의 규모가 되면 데이터는 증가한다. 이 데이터량이 분수령을 넘어서면 문제가 복잡해진다. 그리고 응급처리로는 쉽사리 풀리지 않는다. 이 점이 대규모 서비스의 어려운 점이다.

- 어떻게 하면 데이터를 적게 가져갈 수 있을까?
- 어떻게 하면 여러 서버로 분산시킬 수 있을까?
- 어떻게 하면 필요한 데이터를 최소한의 횟수로 읽어들일 수 있을까?

## 계속 성장하는 서비스와 대규모화의 벽
### 웹 서비스의 어려움
서비스가 계속해서 성장한다.
- 시간이 지날수록 보유하게 되는 데이터의 양도 크게 늘어난다.
- 이전의 데이터를 사업자가 임의로 지워버릴 수 없어 모든 데이터는 계속해서 잘 보존하고 필요한 경우 추출해낼 필요가 있다.
- 상업적으로 성공한 경우엔 데이터뿐만 아니라 트래픽도 늘어난다.

#### 시행착오를 거듭한 시스템 규모확장
하테나의 실제 사례는 다음과 같다.

- 라우터는 Linux 박스로 저가에 구축
- HTTP 요청 분산은 아파치의 `mod_rewrite`로 대용
- DB 분산은 당시 불안정했던 MySQL의 레플리케이션 기능을 조심해서 사용

이런 방식으로 서서히 시스템 규모를 확장하다 어느 순간 트래픽 증가에 비해 시스템 확장이 따라가지 못했다고 한다.

#### 데이터 센터로의 이전, 시스템 쇄신
- 사전에 기존 시스템의 부하상황을 정리
- 이 정보를 활용해 각 서비스의 구성 중 병목지점을 측정, 판정하고 I/O 부하가 높은 서버는 메모리를 중요시하고 CPU 부하가 높은 서버는 CPU를 중요시하는 형태로 서버 용도에 맞게 최적의 구성을 갖는 하드웨어를 준비
- 다중화의 경우 로드밸런서 + 자동감시 기능을 하는 오픈소스를 도입.
- 서버 교체에는 서서히 OS 가상화도 진행해 서버 가동률을 높임과 동시에 유지보수성을 높여갔다.
- 서버의 정보관리를 위해 독자적인 웹 기반 서버 정보관리 시스템도 개발하여 서버의 용도나 부하상태와 같은 각종 정보에 액세스하기 쉬워져 전체 시스템을 파악하기 용이해짐.
- 애플리케이션의 각종 로직이나 DB 스키마 등도 재검토해서 비효율적인 부문을 서서히 배제해감.
- 필요에 따라 검색 엔진 등의 서브시스템을 독자 개발하기도 함.

### 시스템의 성장 전략
서비스가 소규모인 단계에선 심플한 방법이 더 나은 경우가 많으므로 너무 이른 최적화가 좋은 방침이라곤 할 수 없다. 처음부터 완벽한 부하분산 시스템을 구축하려하면 비용이 너무 많이 들게 된다.

대규모화의 벽은 갑작스레 눈앞에 나타난다. 데이터 규모가 증가함에 따른 I/O 부하 상승은 그 정도로 순조롭게 증가하는 것은 아니다. 캐시 미스가 발생하기 시작한 후 오래지 않아 갑자기 문제가 복잡해지므로, 알아차렸을 때는 이미 시스템이 저속화하고 있는 경우가 자주 있다.

이런 사태가 발생하지 않게 하기 위해 어느 정도의 수용능력 관리나 서비스 설계 시 필요 이상으로 데이터를 증가시키지 않도록 하는 설계를 포함시키는 게 좋을 것이다.