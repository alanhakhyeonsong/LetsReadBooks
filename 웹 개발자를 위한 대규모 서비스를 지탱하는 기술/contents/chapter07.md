# Chapter 7 - 알고리즘 실용화
## 알고리즘과 평가
대상이 되는 데이터가 크면 클수록 알고리즘이나 데이터 구조 선택이 속도에 영향을 미친다.
- 선형 탐색
- 이분 탐색

CPU나 메모리 등 컴퓨터의 자원은 유한하므로 알고리즘에 대해 배우는 것은 중요하다. 알고리즘을 배우는 이점으로 가장 이해하기 쉬운 것은, 배워두면 새로운 문제에도 대처할 수 있다는 점일 것이다.
- 베이지안 필터 → 데이터 자동분류, 스팸 메일 필터
- 수억 건의 레코드를 수 MB로 저장할 수 있는 데이터 구조가 있다면, 배포하기 너무 컸던 프로그램을 부담없이 배포할 수 있게 될 것.

### 알고리즘의 실제 활용
실제로는 고도의 알고리즘이 반드시 최고의 해법인 것은 아니며, 고전적인 알고리즘이 좋은 경우도 있다. 또한 잘 알려진 알고리즘보다 단순한 알고리즘이 더 나은 경우도 자주 있다.

- ex) 하테나 북마크 Firefox 확장기능인 검색기능
  - 과거에 사용자 본인이 북마크한 데이터를 증분검색 할 수 있는 기능
  - Suffix Array 사용 → 텍스트 데이터 등을 고속으로 검색하기 위한 데이터구조로 탐색 자체는 빠르지만 미리 전처리를 거친 데이터 구조를 만들어둘 필요가 있고 이 전처리에 상당한 시간이 필요함.
    - IS법을 JavaScript로 구현해서 완성했으나, 실제로 속도는 나지만 전처리 수행시 머신에 큰 부하가 많이 걸렸다.
  - SQL로 `like`에 의한 부분일치 검색(선형 탐색)을 수행하도록 변경 → 실제 완성한 결과 이걸로도 전혀 문제없이 사용할 수 있었다.

정평이 난 알고리즘은 제3자가 이용하기 쉽도록 이미 구현된 소스가 공개되어 있는 경우도 많다. 이런 종류의 소스를 잘 이용하면 공수를 줄일 수 있다. 단, 어느 정도는 구현이 어떻게 되어 있는지를 알아두지 않으면 잘못된 선택을 할 수도 있다는 점에 유의하자.

## 하테나 다이어리의 키워드 링크
### 키워드 링크란?
블로그에 글을 작성하면 일부 키워드에 링크가 자동으로 걸린다. 이 링크가 걸리는 곳은 이 키워드를 설명하는 페이지로 되어 있다. Wiki 구현에서도 마찬가지로 자동으로 링크하는 기능이 있어 이와 비슷한 기능이라 생각하면 된다.

입력된 전문에 대해 27만 단어를 포함하는 키워드 사전과 매칭해서 필요한 부분을 링크로 치환하는 것이 키워드 링크의 기능이다. 링크로 치환하는 작업은 실제로는 특정 키워드를 HTML anchor 태그로 치환하는 것 뿐이므로 문장 내의 키워드 위치를 텍스트 치환하는 문제라고 할 수 있다.

- ex) 하테나 다이어리는 블로그다.
  - `<a href="...">하테나 다이어리</a>는 <a href="...">블로그</a>다.`

### 최초 구현방법
초반에는 정규표현으로 구현하는 단순한 방법을 채택했었다. 사전 내에 포함된 모든 단어를 OR 조건으로 있는 정규표현을 만들어 사용하는 것이다.

```
use URI::Escape;

$text =- s/(foo/bar/baz)/$replace_keyword($1)/ge;

sub replace_keyword {
  my $w = shift;
  return sprint '<a href="/keyword/%s">%s</a>', uri_escape($w), $w;
}
```

### 문제발생 - 키워드 사전의 대규모화
키워드는 사용자가 등록하는 것이므로 오픈 당시엔 그다지 어휘수도 많지 않고, DB에서 그때그때 정규표현을 만들어 키워드 링크를 하는 식의 여유로운 처리로도 문제없이 동작했었다. 그런데 키워드수가 많아짐에 따라 문제가 발생하기 시작했다. 정규표현 처리에 시간이 걸리는 것이다.

- 정규표현을 컴파일하는 처리
  - 미리 정규표현을 만들어 메모리나 디스크 상에 저장해두는, 즉 캐싱으로 회피는 가능함.
- 정규표현에서 패턴매칭하는 처리
  - 새로 추가된 키워드를 키워드 링크에 반영시키기 위해선 일정 시간에 캐시를 다시 구축해야 할 필요가 있거나 그다지 액세스가 없는 블로그에선 캐시가 효과를 나타내기 어려움 등의 근본적인 해결에는 이르지 못함.

키워드 링크에 계산시간이 걸리는 문제의 원인은 **정규표현 알고리즘에 있다.** NFA 정규표현은 패턴매칭을 앞에서부터 입력값을 살펴가면서 매칭에 실패하면 다음 단어를 시도하고, 또 실패하면 그 다음 단어를 시도하는 단순한 방법으로 처리한다. 그 결과, 키워드의 개수에 비례하는 계산량이 소요된다.

### 정규표현 → Trie - 매칭 구현 변경
정규표현을 기반으로 한 방법에서 Trie를 사용한 매칭 구현으로 변경함.

Trie는 트리구조의 일종인 데이터구조다. 탐색대상 데이터의 공통 접두사를 모아 트리구조를 이루는 게 그 특징이다.
- 문자열 집합을 트리구조로 해서 효율적으로 저장한다.
- 탐색대상 데이터의 공통 접두사를 모아놓은 트리구조가 된다.

트라이 구조를 사전과 비교하면서 패턴매칭을 하면 정규표현인 경우보다 계산량을 줄일 수 있다. 입력 문서를 Trie에 입력한 다음, 엣지를 순회하면서 종단이 발견되면 해당 단어가 포함되어 있는 것으로 간주하는 것이다. 이런 방법이라면 공통 접두사는 단 한 번 탐색으로 마칠 수 있게 된다.

`hogefoo`라는 단어를 입력으로 `foo`, `bar`, `baz`의 Trie 구조를 순회하도록 한다면, 입력 단어의 길이만큼만 계산하면 끝이다.

### AC법
- 계산량이 사전 크기에 의존하지 않는 빠른 방법
- 사전 내에서 패턴매칭을 수행하는 오토마톤을 구축하고 입력 텍스트에 대해 선형 계산시간을 실현

처음부터 최적의 구현을 사용하는 것이 반드시 옳다고는 할 수 없다. 데이터가 작은 동안에는 오히려 결과도 좋을 것 같다. 데이터가 대규모가 될 시기를 대비해 본질적인 문제의 해결방법을 머릿속에 넣어두지 않으면 곤란하다.

## 하테나 북마크의 기사 분류
### 기사 분류란?
새로 도착한 기사를 해당 기사의 내용을 기반으로 자동으로 분류해서 사용자에게 카테고리를 분류해 보여주는 기능을 제공하고 있다.

이 카테고리 판정에는 베이지안 필터라는 원리를 사용하고 있다. 베이지안 필터는 스팸필터 등에도 응용되고 있다. 베이지안 필터는 텍스트 문서 등을 입력으로 받아들이고 거기에 나이브 베이즈라고 하는 알고리즘을 적용해 확률적으로 해당 문서가 어느 카테고리에 속하는지를 판정하는 프로그램이다. 미지의 문서의 카테고리 판정을 수행함에 있어 과거에 분류가 끝나느 데이터의 통계정보로부터 판정을 수행한다는 점이다.

이런 처리는 '기계학습' 분야의 연구성과다.

### 기계학습과 대규모 데이터
많은 기계학습 태스크엔 베이지안 필터와 같이 정해 데이터를 필요로 한다. 정해 데이터를 입력으로서 부여한 학습엔진은 사람과 동등하거나 그 이상의 정밀도로 특정 문제를 해결할 수 있게 된다.

기계학습의 태스크에 따라선 데이터가 많으면 많을수록 정밀도가 향상되는 경우도 드물지 않다. 대규모 웹 서비스가 안고 있는 대량의 데이터는 확장성 관점에선 운영을 고민하게 하는 원인이 되기도 하지만, 한편으론 연구개발 분야에선 몹시 탐이 나는 데이터이기도 하다.

### 관련 엔트리
- 추천 알고리즘
- ex) Elasticsearch로 추천 서비스 및 검색 플랫폼 개발

### 베이지안 필터의 원리
특정 문서 D가 주어졌을 때 이 문서가 확률적으로 어떤 카테고리 C에 속하는 게 가장 그럴듯한가?

- P(C|D) = P(D|C) P(C) / P(D)

결과적으로 P(D|C), P(D)만 알면 되는 문제로 귀착된다.