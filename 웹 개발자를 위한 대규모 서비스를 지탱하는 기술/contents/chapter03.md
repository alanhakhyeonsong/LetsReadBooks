# Chapter 3 - OS 캐시와 분산
## OS의 캐시 구조
- 디스크와 메모리 간 속도차는 10^5 ~ 10^6배 이상
- OS는 메모리를 이용해 디스크 액세스를 줄인다.
  - OS는 캐시 구조를 갖추고 있다.

OS는 '가상 메모리 구조'를 갖추고 있다. 이는 논리적인 선형 어드레스를 물리적인 물리 어드레스로 변환하는 것이다.

### 가상 메모리 구조
![](https://github.com/alanhakhyeonsong/LetsReadBooks/assets/60968342/46c11a97-6b3d-4e66-96b4-b2786e12bc8b)

- 프로세스에서 메모리를 다루기 쉽게 하는 이점을 제공한다.
- OS가 커널 내에서 메모리를 추상화하고 있다.
- 페이지: OS가 물리 메모리를 확보/관리하는 단위

### Linux의 페이지 캐시 원리
![](https://github.com/alanhakhyeonsong/LetsReadBooks/assets/60968342/363e53be-5327-429f-b718-63f8bc85c5bb)

- 디스크의 내용을 일단 메모리에 읽어들인다.
  - 페이지가 작성된다.
- 작성된 페이지는 파기되지 않고 남는다.
  - 페이지 캐시
- 예외의 경우를 제외하고 모든 I/O에 투과적으로 작용한다.
  - 디스크의 캐시를 담당하는 곳(VFS)

### VFS
디스크의 캐시는 페이지 캐시에 의해 제공되지만, 실제 이 디스크를 조작하는 디스크 드라이버와 OS 사이에는 파일 시스템이 끼어 있다. 파일 시스템 위에는 VFS(Virtual File System)이라는 추상화 레이어가 있다. 파일 시스템은 다양한 함수를 갖추고 있는데, 그 인터페이스를 통일하는 것이 VFS의 역할이다. 또한 VFS가 페이지 캐시의 구조를 지니고 있다. 어떤 파일 시스템을 이용하더라도, 어떤 디스크를 읽어내더라도 반드시 동일한 구조로 캐싱된다.

### Linux는 페이지 단위로 디스크를 캐싱한다
![](https://github.com/alanhakhyeonsong/LetsReadBooks/assets/60968342/8a9c587e-9d45-477c-9f89-bfc615c2f5fd)

- 페이지 = 가상 메모리의 최소 단위

#### LRU
메모리 여유분이 1.5GB 있고 파일을 4GB 전부 읽게 되면 어떻게 될까?  
구조상으로는 LRU 형태로 되어 있으므로 최근 읽은 부분이 캐시에 남고 과거에 읽은 부분이 파기되어 간다. 따라서 DB도 계속 구동시키면 캐시가 점점 최적화되어 가므로 기동시킨 직후보다 점점 뒤로 갈수록 부하, I/O가 내려가는 특성을 보인다.

### 메모리가 비어 있으면 캐싱
Linux는 메모리가 비어 있으면 전부 캐싱한다. 여기엔 제한이 없어 Linux는 비어 있는 메모리 공간에 계속해서 디스크 내용을 캐싱해간다. 한편 프로세스에서 메모리를 요청했을 때 캐시로 인해 더 이상 메모리가 남아있지 않다면 오래된 캐시를 버리고 프로세스에 메모리를 확보해준다.

```bash
$ sar -r 1 10000

# 생략
19:50:32  kbmemfree kbmemused %memused kbbuffers kbcached %swpused kbswpcad
19:50:33       5800   1005888    99.43     28244   694068     0.00        0
19:50:34       5800   1005888    99.43     28244   694068     0.00        0
19:50:35       5800   1005888    99.43     28244   694068     0.00        0
19:50:36       5800   1005888    99.43     28244   694068     0.00        0
```

`kbcached`는 kilo byte cached의 약자로, 캐싱되어 있는 용량이다. 현재 시스템은 대략 1GB의 메모리를 갖고 있고 그중 694MB, 700MB 가까이 캐시에 사용되고 있다. `%memused`를 보면 메모리를 99% 정도 사용하고 있다.

실제로는 메모리가 비어 있는 곳에 OS가 조금씩 디스크를 캐싱하고 있는 것일 뿐이다.

캐시 이외에 메모리가 필요해지면 오래된 캐시가 파기된다.

### 메모리를 늘려서 I/O 부하 줄이기
지금까지의 내용에 따르면 **메모리를 늘리면 실제 I/O 부하를 줄일 수 있음을 알 수 있다.**
- 메모리를 늘리면 캐시에 사용할 수 있는 용량이 늘어남.
- 캐시에 사용할 수 있는 용량이 늘어나면 보다 많은 데이터를 캐싱할 수 있다.
- 많이 캐싱되면 디스크를 읽는 횟수가 줄어든다.

```bash
$ sar -f /var/log/sa/sa05
14:10:01  CPU %user %nice %system %iowait %idle
14:20:01  all  8.58  0.00    5.84   16.58 69.00
14:30:01  all  7.41  0.00    5.14   17.81 69.63
14:40:01  all  7.74  0.00    4.97   18.56 68.73
14:50:01  all  7.02  0.00    5.01   16.24 71.72
```

`%iowait`가 대략 20% 정도다. 이는 프로세스가 작업을 수행할 때 항상 I/O에서 대기를 한다는 신호다. 이는 그다지 좋지 않다.

메모리를 8GB로 늘린 결과 아래와 같이 대기가 거의 없어졌다.

```bash
$ sar -f /var/log/sa/sa05
14:10:01  CPU %user %nice %system %iowait %idle
14:20:01  all 18.16  0.00   11.56    0.80 69.49
14:30:01  all 12.48  0.00    9.47    0.88 77.17
14:40:01  all 14.20  0.00   10.17    0.91 74.72
14:50:01  all 13.25  0.00    9.74    0.75 76.25
```

이것이 의미하는 것은 4GB에선 전부 캐싱할 수 없었으나 8GB로 늘리고 나니 데이터베이스상의 파일을 대부분 캐시로 올릴 수 있었다는 것이다.

이와 같이 메모리를 늘려 I/O 부하를 줄이자는 것이 데이터가 많아졌을 때의 기본 방침이다.

### 페이지 캐시는 투과적으로 작용한다
```bash
$ sar -r 1 10000

# 생략
18:20:01  kbmemfree kbmemused %memused kbbuffers kbcached %swpused kbswpcad
18:30:01    3566992    167272     4.42     11224    50136     0.00        0
18:40:01    3546264    178000     4.78     12752    66548     0.00        0
18:50:01     112628   3611636    96.98      4312  3499144     0.00       44
```

갑자기 메모리 사용량이 96.98%로 올라가고있다. 순간 매우 큰 파일을 read한 것이다. 이것이 전부 캐시에 저장되어 96%를 사용하게 되었다. OS 부팅 직후에는 커널이 디스크를 그다지 읽지 않았으므로 캐시로 데이터가 거의 유입되지 않았지만, 특정 파일을 read하면 이를 쭉 캐싱해가는 것이다. 파일을 캐싱하는 원리는 대략 이런 형태로 되어 있다.

## I/O 부하를 줄이는 방법
### 캐시를 전제로 한 I/O 줄이는 방법
- '데이터 규모 < 물리 메모리'이면 전부 캐싱할 수 있다.
- 경제적 비용과의 밸런스 고려

### 복수 서버로 확장시키기 - 캐시로 해결될 수 없는 규모일 경우
- CPU 부하분산에는 단순히 늘린다.
- I/O 분산에는 국소성을 고려한다.

### 단순히 대수만 늘려서는 확장성을 확보할 수 없다
단순히 데이터를 복사해서 대수를 늘리게 되면 애초에 캐시 용량이 부족해 늘렸는데 그 부족한 부분도 그대로 동일하게 늘려가게 되는 것이다. 곧 다시 병목이 된다.

## 국소성을 살리는 분산
- 액세스 패턴을 고려한 분산
- 캐싱할 수 없는 부분이 사라진다.
  - 메모리는 디스크보다 빠르므로 그만큼 덕을 본다.

### 파티셔닝
파티셔닝은 한 대였던 DB 서버를 여러 대의 서버로 분할하는 방법을 말한다. 여러 가지 방법이 있지만, 간단한 것은 '테이블 단위 분할'이다.

- 파티셔닝 1 → 같이 액세스하는 경우가 많으므로 같은 서버에 위치
  - entry 테이블
  - bookmark 테이블
- 파티셔닝 2
  - tag 테이블
  - keyword 테이블

크기로는 개당 2GB 정도의 테이블이 여러 개, 대략 16GB 정도의 메모리를 탑재한 머신을 준비해두면 전부 메모리에 올릴 수 있다.

tag, keyword 테이블은 각각 꽤 커서 10GB 정도 된다. 1번 파티셔닝 서버에 같이 저장하게되면 전부 캐싱할 수 없게 된다.

테이블 단위로 분할했으면 각 테이블로의 요청은 각 테이블이 위치한 서버로 보내 처리될 수 있도록 애플리케이션을 변경할 필요가 있다.

다른 분할 방법으로는 '테이블 데이터 분할'이다.
- id 기준에 따라 서버 분할

### 요청 패턴을 '섬'으로 분할
조금 독특하지만, '용도별로 시스템을 섬으로 나누는 방법'도 있다.

HTTP 요청의 User-Agent나 URL 등을 보고, 섬으로 나누는 방법을 사용한다. 봇 이외의 액세스, 즉 사용자로부터의 액세스는 최상위 페이지나 인기 엔트리 페이지 등 최신, 인기 페이지에 거의 액세스가 집중되므로 빈번하게 참조되는 부분은 캐싱하기 쉽다.

이렇게 해서 캐싱하기 쉬운 요청, 캐싱하기 어려운 요청을 처리하는 섬을 나누게 되면, 전자는 국소성으로 인해 안정되고 높은 캐시 적중률을 낼 수 있게 된다. 후자의 요청이 전자의 캐시를 어지럽히므로 섬으로 나누는 경우에 비해 전체적으로는 캐시 효율이 떨어진다.

### 페이지 캐시를 고려한 운용의 기본 규칙
- OS 기동 후에 서버를 곧바로 투입하지 않는다.
- 성능평가는 캐시가 최적화되었을 때